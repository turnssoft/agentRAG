# ────────────────────────────────────────────────────────────────────────────
# -- Chat Settings ──────────────────────────────────────────────────────────

# ─── Chat host ─────────────────────────────────────────────────────────────
LLM_HOST=LLM_STUDIO

# ─── Chat host llama.cpp ────────────────────────────────────────────────────
LLM_HOST_LLAMA_CPP=http://localhost:12434
LLM_PATH_LLAMA_CPP=engines/llama.cpp/v1/chat/completions
OLLAMA_MODEL_LLAMA_CPP=ai/llama3.2:latest

# ─── Chat host LLM Studio ────────────────────────────────────────────────────
LLM_HOST_LLM_STUDIO=http://localhost:1234
LLM_PATH_LLM_STUDIO=v1/chat/completions
# models: 
# liquid/lfm2-1.2b
# llama-3.2-3b-instruct
OLLAMA_MODEL_LLM_STUDIO=llama-3.2-3b-instruct

# ─── System & Prompt ────────────────────────────────────────────────────────
PROMPT_FILE=prompt.txt
ROLE_SYSTEM_CONTENT="You are a helpful assistant."


# ────────────────────────────────────────────────────────────────────────────
# -- Embedding Settings ──────────────────────────────────────────────────────


# ─── Embedding host ────────────────────────────────────────────────────────
EMBEDDING_HOST=LLM_STUDIO

# ─── Embedding host llama.cpp ─────────────────────────────────────────────
EMBEDDING_HOST_LLAMA_CPP=http://localhost:12434
EMBEDDING_PATH_LLAMA_CPP=llama.cpp/v1/embeddings
EMBEDDING_MODEL_LLAMA_CPP=ai/mxbai-embed-large
EMBEDDING_DIMENSION_LLAMA_CPP=1024

# ─── Embedding host LLM Studio ─────────────────────────────────────────────
EMBEDDING_HOST_LLM_STUDIO=http://localhost:1234
EMBEDDING_PATH_LLM_STUDIO=v1/embeddings
EMBEDDING_MODEL_LLM_STUDIO=nomic-embed-text-v1.5
EMBEDDING_DIMENSION_LLM_STUDIO=768

# ─── Vector store rebuild toggle ───────────────────────────────────────────
REBUILD_VECTOR=false

# ─── ChromaDB backend engine ───────────────────────────────────────────────
DB_LOCATION="./faiss_index"
COLLECTION_NAME="PizzaBoys"

# ─── Data folder ────────────────────────────────────────────────────────────
DATA_FOLDER="./data"

# ─── Splitter settings ──────────────────────────────────────────────────────
CHUNK_SIZE=500
CHUNK_OVERLAP=100

# ─── Top K for retrieval ───────────────────────────────────────────────────
RETRIEVAL_TOP_K=10


